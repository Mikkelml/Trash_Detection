{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import EfficientNetB0, ResNet50, MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transfer_learning_model(base_model_name='EfficientNetB0', \n",
    "                                 input_shape=(256, 256, 3), \n",
    "                                 num_classes=9,\n",
    "                                 freeze_base=True):\n",
    "    \"\"\"\n",
    "    Create a transfer learning model using ImageNet pre-trained weights\n",
    "    \n",
    "    Args:\n",
    "        base_model_name: Choice of base model ('EfficientNetB0', 'ResNet50', 'MobileNetV2')\n",
    "        input_shape: Input image shape\n",
    "        num_classes: Number of classes to classify\n",
    "        freeze_base: Whether to freeze base model weights initially\n",
    "    \n",
    "    Returns:\n",
    "        Compiled Keras model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Choose base model\n",
    "    if base_model_name == 'EfficientNetB0':\n",
    "        base_model = EfficientNetB0(\n",
    "            weights='imagenet',  # Use ImageNet weights\n",
    "            include_top=False,   # Don't include final classification layer\n",
    "            input_shape=input_shape\n",
    "        )\n",
    "    elif base_model_name == 'ResNet50':\n",
    "        base_model = ResNet50(\n",
    "            weights='imagenet',\n",
    "            include_top=False,\n",
    "            input_shape=input_shape\n",
    "        )\n",
    "    elif base_model_name == 'MobileNetV2':\n",
    "        base_model = MobileNetV2(\n",
    "            weights='imagenet',\n",
    "            include_top=False,\n",
    "            input_shape=input_shape\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Choose from 'EfficientNetB0', 'ResNet50', or 'MobileNetV2'\")\n",
    "    \n",
    "    # Freeze base model layers initially\n",
    "    if freeze_base:\n",
    "        base_model.trainable = False\n",
    "        print(f\"Base model ({base_model_name}) frozen - {len(base_model.layers)} layers\")\n",
    "    else:\n",
    "        print(f\"Base model ({base_model_name}) unfrozen - {len(base_model.layers)} layers\")\n",
    "    \n",
    "    # Add custom classification head\n",
    "    model = tf.keras.Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        \n",
    "        # First dense layer\n",
    "        Dense(512, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        # Second dense layer\n",
    "        Dense(256, activation='relu'), \n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # Output layer\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model, base_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_transfer_model(model, learning_rate=0.001):\n",
    "    \"\"\"Compile the transfer learning model\"\"\"\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy', 'top_3_accuracy']\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_callbacks(model_name='best_transfer_model.keras'):\n",
    "    \"\"\"Create training callbacks\"\"\"\n",
    "    return [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=15,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.2,\n",
    "            patience=5,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            model_name,\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and compare different models\n",
    "def compare_models(train_generator, val_generator, class_weights):\n",
    "    \"\"\"Compare different pre-trained models\"\"\"\n",
    "    \n",
    "    models_to_test = ['EfficientNetB0', 'ResNet50', 'MobileNetV2']\n",
    "    results = {}\n",
    "    \n",
    "    for model_name in models_to_test:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training {model_name}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Create model\n",
    "        model, base_model = create_transfer_learning_model(\n",
    "            base_model_name=model_name,\n",
    "            input_shape=(256, 256, 3),\n",
    "            num_classes=9,\n",
    "            freeze_base=True\n",
    "        )\n",
    "        \n",
    "        # Compile model\n",
    "        model = compile_transfer_model(model, learning_rate=0.001)\n",
    "        \n",
    "        # Print model summary\n",
    "        print(f\"\\nModel Summary for {model_name}:\")\n",
    "        model.summary()\n",
    "        \n",
    "        # Train model\n",
    "        callbacks = create_callbacks(f'best_{model_name.lower()}_model.keras')\n",
    "        \n",
    "        history = model.fit(\n",
    "            train_generator,\n",
    "            epochs=30,  # Start with fewer epochs for transfer learning\n",
    "            validation_data=val_generator,\n",
    "            class_weight=class_weights,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        val_loss, val_accuracy, val_top3 = model.evaluate(val_generator, verbose=0)\n",
    "        \n",
    "        results[model_name] = {\n",
    "            'model': model,\n",
    "            'base_model': base_model,\n",
    "            'history': history,\n",
    "            'val_accuracy': val_accuracy,\n",
    "            'val_loss': val_loss,\n",
    "            'val_top3': val_top3\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{model_name} Results:\")\n",
    "        print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "        print(f\"Top-3 Accuracy: {val_top3:.4f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tunning chosen model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning function (advanced technique)\n",
    "def fine_tune_model(model, base_model, train_generator, val_generator, \n",
    "                   class_weights, fine_tune_at=100):\n",
    "    \"\"\"\n",
    "    Fine-tune the model by unfreezing some layers of the base model\n",
    "    \n",
    "    Args:\n",
    "        model: The trained model\n",
    "        base_model: The base pre-trained model\n",
    "        fine_tune_at: Layer index from which to start fine-tuning\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nFine-tuning from layer {fine_tune_at}...\")\n",
    "    \n",
    "    # Unfreeze the top layers of the model\n",
    "    base_model.trainable = True\n",
    "    \n",
    "    # Freeze all layers except the top ones\n",
    "    for layer in base_model.layers[:fine_tune_at]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Recompile with lower learning rate for fine-tuning\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0001),  # Lower learning rate\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy', 'top_3_accuracy']\n",
    "    )\n",
    "    \n",
    "    print(f\"Trainable layers: {sum([1 for layer in model.layers if layer.trainable])}\")\n",
    "    \n",
    "    # Continue training\n",
    "    fine_tune_callbacks = create_callbacks('fine_tuned_model.h5')\n",
    "    \n",
    "    fine_tune_history = model.fit(\n",
    "        train_generator,\n",
    "        epochs=20,  # Fewer epochs for fine-tuning\n",
    "        validation_data=val_generator,\n",
    "        class_weight=class_weights,\n",
    "        callbacks=fine_tune_callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return fine_tune_history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run chosen model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model (EfficientNetB0 is recommended for your use case)\n",
    "model, base_model = create_transfer_learning_model(\n",
    "    base_model_name='EfficientNetB0',\n",
    "    input_shape=(256, 256, 3),\n",
    "    num_classes=9,\n",
    "    freeze_base=True\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "model = compile_transfer_model(model, learning_rate=0.001)\n",
    "\n",
    "# Print model info\n",
    "print(f\"\\nModel Architecture:\")\n",
    "model.summary()\n",
    "\n",
    "print(f\"\\nTotal parameters: {model.count_params():,}\")\n",
    "trainable_params = sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Non-trainable parameters: {model.count_params() - trainable_params:,}\")\n",
    "\n",
    "# Training setup\n",
    "callbacks = create_callbacks('best_efficientnet_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train transfer learning model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=50,  # Usually needs fewer epochs than training from scratch\n",
    "    validation_data=val_generator,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# After training, optionally fine-tune\n",
    "# fine_tune_history = fine_tune_model(model, base_model, train_generator, val_generator, class_weights)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "def plot_transfer_history(history):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[0, 0].plot(history.history['accuracy'], label='Training')\n",
    "    axes[0, 0].plot(history.history['val_accuracy'], label='Validation') \n",
    "    axes[0, 0].set_title('Model Accuracy')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid()\n",
    "    \n",
    "    # Loss\n",
    "    axes[0, 1].plot(history.history['loss'], label='Training')\n",
    "    axes[0, 1].plot(history.history['val_loss'], label='Validation')\n",
    "    axes[0, 1].set_title('Model Loss')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid()\n",
    "    \n",
    "    # Top-3 Accuracy\n",
    "    axes[1, 0].plot(history.history['top_3_accuracy'], label='Training')\n",
    "    axes[1, 0].plot(history.history['val_top_3_accuracy'], label='Validation')\n",
    "    axes[1, 0].set_title('Top-3 Accuracy')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid()\n",
    "    \n",
    "    # Learning Rate\n",
    "    if 'lr' in history.history:\n",
    "        axes[1, 1].plot(history.history['lr'])\n",
    "        axes[1, 1].set_title('Learning Rate')\n",
    "        axes[1, 1].set_yscale('log')\n",
    "        axes[1, 1].grid()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
